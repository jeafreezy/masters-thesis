{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "Y52INKUEHvSa"
         },
         "source": [
            "## Package Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "id": "dFZKrGTDHvSa"
         },
         "outputs": [],
         "source": [
            "import os\n",
            "import random\n",
            "\n",
            "# Data Analysis\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import numpy as np\n",
            "\n",
            "# Visualization\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "\n",
            "# Stats & ML\n",
            "\n",
            "from sklearn.ensemble import IsolationForest\n",
            "\n",
            "from sklearn.impute import KNNImputer\n",
            "\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
            "from sklearn.model_selection import TimeSeriesSplit\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.impute import SimpleImputer\n",
            "import polars as pl\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "\n",
            "pl.Config.set_tbl_rows(-1)\n",
            "%matplotlib inline"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "hyDedPc8HvSa"
         },
         "source": [
            "## Data Loading"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Load all water meter devices"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "2Xu83Mf6rT51",
            "outputId": "a9349bdc-5f4a-441a-b85c-6b299e155280"
         },
         "outputs": [],
         "source": [
            "try:\n",
            "    df = pl.read_csv('../data/Abyei_water_meters.csv', infer_schema_length=100000)\n",
            "    print(f\"Data loaded successfully\")\n",
            "except Exception as e:\n",
            "    print(f\"An error occurred while loading the data: {e}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Load devices to enhance"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "try:\n",
            "    poor_devices = pl.read_csv('../exports/devices_to_clean.csv', infer_schema_length=100000)\n",
            "    print(f\"Data loaded successfully\")\n",
            "except Exception as e:\n",
            "    print(f\"An error occurred while loading the data: {e}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "poor_devices.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Filter out the data for these devices from the main df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Get the list of device IDs from the filtered devices\n",
            "filtered_device_ids = poor_devices['DEVICE_ID'].to_list()\n",
            "\n",
            "filtered_data = df.filter(pl.col('DEVICE_ID').is_in(filtered_device_ids))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "filtered_data.shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "id": "qQTpWhjrlUTC"
         },
         "outputs": [],
         "source": [
            "level_1_devices = filtered_data.filter(pl.col('OGI_LEVEL') == 1)['MISSION_DEVICE_TAG'].n_unique()\n",
            "level_2_devices = filtered_data.filter(pl.col('OGI_LEVEL') == 2)['MISSION_DEVICE_TAG'].n_unique()\n",
            "level_3_devices= filtered_data.filter(pl.col('OGI_LEVEL') == 3)['MISSION_DEVICE_TAG'].n_unique()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "dQoAEzAdsnu9",
            "outputId": "e82b79eb-28fe-4b98-e626-f923e9cf0ea4"
         },
         "outputs": [],
         "source": [
            "level_1_devices , level_2_devices, level_3_devices"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "All the poor devices are in level 2."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "id": "3uV4cozEHvSn"
         },
         "source": [
            "#### How many devices are available in total?"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "XVPLp0QzHvSn",
            "outputId": "55d58436-e3ea-4db1-f1c4-f9ff9a183781",
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "filtered_data['DEVICE_ID'].n_unique()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "filtered_data.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Convert date string to date object"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "filtered_data = filtered_data.with_columns(\n",
            "    pl.col(\"TAG_VALUE_DATE\").cast(pl.Datetime)\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "filtered_data.sort('DEVICE_ID').head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Daily resample"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cummulative_daily_consumption = (\n",
            "    filtered_data\n",
            "    .with_columns(\n",
            "        pl.col(\"TAG_VALUE_DATE\").dt.truncate(\"1d\").cast(pl.Date).alias(\"DATE\")\n",
            "    )\n",
            "    .sort([\"DEVICE_ID\", \"DATE\"]) \n",
            "    .group_by([\"DEVICE_ID\", \"DATE\"])\n",
            "    .agg([\n",
            "        pl.col(\"TAG_VALUE_RAW\").max().alias(\"CUMMULATIVE_CONSUMPTION\"),\n",
            "        pl.col(\"OGI_LONG\").first(),\n",
            "        pl.col(\"OGI_LAT\").first(),\n",
            "    ])\n",
            "    .sort([\"DEVICE_ID\", \"DATE\"])\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cummulative_daily_consumption.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cummulative_daily_consumption.write_csv('../exports/abyei_to_clean_complete.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cummulative_daily_consumption.shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select 5 random devices\n",
            "random_devices = random.sample(cummulative_daily_consumption['DEVICE_ID'].unique().to_list(), 5)\n",
            "\n",
            "# Create the plot\n",
            "plt.figure(figsize=(12, 6))\n",
            "\n",
            "for device_id in random_devices:\n",
            "  device_data = cummulative_daily_consumption.filter(pl.col(\"DEVICE_ID\") == device_id)\n",
            "  device_data_pd = device_data.to_pandas()\n",
            "  plt.plot(device_data_pd['DATE'], device_data_pd['CUMMULATIVE_CONSUMPTION'], label=f'Device {device_id}')\n",
            "\n",
            "plt.xlabel('Date')\n",
            "plt.ylabel('Cummulative Daily Consumption')\n",
            "plt.title('Cummulative Daily Consumption of 5 Random Devices')\n",
            "plt.legend()\n",
            "plt.xticks(rotation=45)\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Validity"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "daily_diff = cummulative_daily_consumption.with_columns(\n",
            "        (pl.col(\"CUMMULATIVE_CONSUMPTION\") - pl.col(\"CUMMULATIVE_CONSUMPTION\").shift(1))\n",
            "        .over(\"DEVICE_ID\")\n",
            "        .alias(\"DAILY_DIFF\")\n",
            "    ).filter(pl.col(\"DAILY_DIFF\").is_not_null())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 1: Flag invalid rows based on business rules\n",
            "daily_diff = daily_diff.with_columns(\n",
            "    pl.when(\n",
            "        (pl.col(\"CUMMULATIVE_CONSUMPTION\") == 0) | (pl.col(\"DAILY_DIFF\") < 0)\n",
            "    )\n",
            "    .then(0)  # Flag as True/1 if invalid\n",
            "    .otherwise(1)  # Otherwise False / 0\n",
            "    .alias(\"VALIDITY\")\n",
            ")\n",
            "daily_diff.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "daily_diff['VALIDITY'].value_counts()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Validity Enhancement"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def correct_detected_resets(df: pl.DataFrame, device_col: str, date_col: str, consumption_col: str):\n",
            "    # Sort data\n",
            "    df = df.sort([device_col, date_col])\n",
            "\n",
            "\n",
            "    # Combine invalid records to detect resets (negative increments or zero cumulative)\n",
            "    df = df.with_columns(\n",
            "        (pl.col('VALIDITY')==0).cast(pl.Boolean).alias('reset_flag')\n",
            "    )\n",
            "\n",
            "    print(f\"Number of resets detected: {df['reset_flag'].sum()}\")\n",
            "\n",
            "    def correct_consumption(df):\n",
            "      reset_indices = df.filter(pl.col('reset_flag')).select([date_col]).to_series()\n",
            "\n",
            "      for reset_date in reset_indices:\n",
            "          # Get the last valid cumulative value before the reset\n",
            "          prev_value = (\n",
            "            df.filter(pl.col(date_col) < reset_date)\n",
            "            .sort(date_col, descending=True)\n",
            "            .select(consumption_col)  # Select only the consumption value\n",
            "            .limit(1)  # Get the most recent entry\n",
            "            .to_series()\n",
            "          )\n",
            "          if len(prev_value) > 0:\n",
            "              prev_value = prev_value[0]\n",
            "\n",
            "              # Calculate the shift required to correct the reset\n",
            "              reset_value = df.filter(pl.col(date_col) == reset_date).select(pl.col(consumption_col)).to_series()[0]\n",
            "              shift_value = prev_value - reset_value\n",
            "\n",
            "              # Apply the shift to all subsequent points\n",
            "              df = df.with_columns(\n",
            "                  pl.when(pl.col(date_col) >= reset_date)\n",
            "                  .then(pl.col(consumption_col) + shift_value)\n",
            "                  .otherwise(pl.col(consumption_col))\n",
            "                  .alias(consumption_col)\n",
            "              )\n",
            "      return df\n",
            "\n",
            "    # Apply correction to dataframe\n",
            "    df = df if df['reset_flag'].sum() == 0 else correct_consumption(df)\n",
            "\n",
            "    return df\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_validity_reset_correction(original_df, corrected_df, device_id):\n",
            "    plt.figure(figsize=(12, 6))\n",
            "\n",
            "    # Plot original cumulative consumption\n",
            "    plt.plot(original_df['DATE'], original_df['CUMMULATIVE_CONSUMPTION'], label='Original', color='blue')\n",
            "\n",
            "    # Plot corrected cumulative consumption\n",
            "    plt.plot(corrected_df['DATE'], corrected_df['CUMMULATIVE_CONSUMPTION'], label='Corrected', color='green')\n",
            "\n",
            "    # Highlight reset points (where VALIDITY == False)\n",
            "    invalid_points = original_df.filter(pl.col('VALIDITY') == False)\n",
            "\n",
            "    plt.scatter(\n",
            "        invalid_points['DATE'],\n",
            "        invalid_points['CUMMULATIVE_CONSUMPTION'],\n",
            "        color='red',\n",
            "        label='Invalid Data (Reset Detected)'\n",
            "    )\n",
            "\n",
            "    plt.title(f'Cumulative Consumption - Device {device_id}')\n",
            "    plt.xlabel('Date')\n",
            "    plt.ylabel('Cumulative Consumption')\n",
            "    plt.legend()\n",
            "    plt.grid(True)\n",
            "    plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "corrected_dataframes = []\n",
            "\n",
            "for device_id in daily_diff['DEVICE_ID'].unique():\n",
            "    device_df = daily_diff.filter(pl.col('DEVICE_ID') == device_id)\n",
            "\n",
            "    device_df = device_df.with_columns(\n",
            "        pl.col('CUMMULATIVE_CONSUMPTION').alias('ORIGINAL_CUMMULATIVE')\n",
            "    )\n",
            "    # Correct cumulative consumption (already part of your loop)\n",
            "    corrected_device_df = correct_detected_resets(\n",
            "      device_df,\n",
            "      device_col='DEVICE_ID',\n",
            "      date_col='DATE',\n",
            "      consumption_col='CUMMULATIVE_CONSUMPTION'\n",
            "    )\n",
            "\n",
            "    # Append to the list for concatenation later\n",
            "    corrected_dataframes.append(corrected_device_df)\n",
            "\n",
            "# Concatenate all corrected dataframes into a single dataframe\n",
            "corrected_cum_daily_consumption = pl.concat(corrected_dataframes)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corrected_cum_daily_consumption.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corrected_cum_daily_consumption.tail()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corrected_cum_daily_consumption.write_csv('../exports/corrected_cum_daily_consumption.csv')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Visualization and exports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# Function to plot original vs reconstructed cumulative consumption\n",
            "def plot_comparison(corrected_data, device_id):\n",
            "    plt.figure(figsize=(12, 6))\n",
            "\n",
            "    # Extract data for the selected device\n",
            "    device_data = corrected_data.filter(pl.col(\"DEVICE_ID\") == device_id).sort(\"DATE\")\n",
            "\n",
            "    # Identify resets (sharp drops in cumulative consumption)\n",
            "    resets = device_data.filter(pl.col(\"VALIDITY\") == 0)  # Assuming resets are marked as invalid\n",
            "\n",
            "    # Plot original cumulative consumption (before reconstruction)\n",
            "    plt.plot(\n",
            "        device_data[\"DATE\"], \n",
            "        device_data[\"ORIGINAL_CUMMULATIVE\"], \n",
            "        color=\"red\", \n",
            "        linestyle=\"-\", \n",
            "        linewidth=1.5, \n",
            "        label=\"Original Cumulative\"\n",
            "    )\n",
            "\n",
            "    # Plot reconstructed cumulative consumption (after reset correction)\n",
            "    plt.plot(\n",
            "        device_data[\"DATE\"], \n",
            "        device_data[\"CUMMULATIVE_CONSUMPTION\"], \n",
            "        color=\"green\", \n",
            "        linestyle=\"-\", \n",
            "        linewidth=1.5, \n",
            "        label=\"Reconstructed Cumulative\"\n",
            "    )\n",
            "\n",
            "    # Add reset annotations\n",
            "    for row in resets.iter_rows(named=True):  # Corrected iteration\n",
            "        plt.scatter(\n",
            "            row[\"DATE\"], \n",
            "            row[\"ORIGINAL_CUMMULATIVE\"], \n",
            "            color=\"black\", \n",
            "            marker=\"o\", \n",
            "            label=\"Reset Point\" if \"Reset Point\" not in plt.gca().get_legend_handles_labels()[1] else \"\",  \n",
            "            zorder=3\n",
            "        )\n",
            "        plt.annotate(\n",
            "            \"Reset\", \n",
            "            xy=(row[\"DATE\"], row[\"ORIGINAL_CUMMULATIVE\"]), \n",
            "            xytext=(row[\"DATE\"], row[\"ORIGINAL_CUMMULATIVE\"] + 8000),  # Adjust annotation position\n",
            "            arrowprops=dict(facecolor='black', arrowstyle=\"->\", lw=1.5),\n",
            "            fontsize=10,\n",
            "            color=\"black\"\n",
            "        )\n",
            "\n",
            "    # Add labels and legend\n",
            "    plt.title(f\"Cumulative Consumption Reconstruction for Device {device_id}\", fontsize=14)\n",
            "    plt.xlabel(\"Date\", fontsize=12)\n",
            "    plt.ylabel(\"Cumulative Consumption\", fontsize=12)\n",
            "    plt.legend(fontsize=10)\n",
            "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
            "    # plt.tight_layout()\n",
            "    plt.savefig(\"../visualizations/plots/cumulative_consumption_reconstruction.png\", dpi=300, bbox_inches=\"tight\")\n",
            "    # Show the plot\n",
            "    plt.show()\n",
            "\n",
            "# Example usage\n",
            "plot_comparison(corrected_cum_daily_consumption, 1187)  # Replace with actual dataset variable\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# Sample 20 random devices\n",
            "sampled_devices = np.random.choice(\n",
            "    corrected_cum_daily_consumption['DEVICE_ID'].unique(),\n",
            "    size=10,\n",
            ")\n",
            "\n",
            "# Plotting function for daily increments\n",
            "def plot_device_increments(df, device_id):\n",
            "    device_data = df.filter(pl.col('DEVICE_ID') == device_id)\n",
            "\n",
            "    plt.figure(figsize=(12, 6))\n",
            "    plt.plot(\n",
            "        device_data['DATE'],\n",
            "        device_data['CUMMULATIVE_CONSUMPTION'],\n",
            "        label=f'Device {device_id}',\n",
            "        color='blue'\n",
            "    )\n",
            "\n",
            "    plt.title(f'Daily Cummulative Consumption for Device {device_id}')\n",
            "    plt.xlabel('Date')\n",
            "    plt.ylabel('Daily Consumption')\n",
            "    plt.legend()\n",
            "    plt.grid(True)\n",
            "    plt.show()\n",
            "\n",
            "# Plot for 20 random devices\n",
            "for device_id in sampled_devices:\n",
            "    plot_device_increments(corrected_cum_daily_consumption, device_id)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Count the number of devices with at least one reset\n",
            "reset_counts = (\n",
            "    corrected_cum_daily_consumption\n",
            "    .filter(pl.col('reset_flag') == True)  # Filter devices with resets\n",
            "    .group_by('DEVICE_ID')\n",
            "    .count()\n",
            ")\n",
            "\n",
            "# Total number of devices that experienced a reset\n",
            "total_devices_reset = reset_counts.height\n",
            "total_devices = len(daily_diff['DEVICE_ID'].unique())\n",
            "total_devices_without_reset = total_devices - total_devices_reset\n",
            "\n",
            "# Plotting\n",
            "plt.figure(figsize=(8, 6))\n",
            "bars = plt.bar(\n",
            "    ['Devices with Reset', 'Devices without Reset'], \n",
            "    [total_devices_reset, total_devices_without_reset],\n",
            "    color=['red', 'blue']\n",
            ")\n",
            "\n",
            "# Add text labels on bars\n",
            "for bar in bars:\n",
            "    height = bar.get_height()\n",
            "    plt.text(\n",
            "        bar.get_x() + bar.get_width() / 2,  # X-position\n",
            "        height,  # Y-position (above bar)\n",
            "        f'{height}',  # Label text\n",
            "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
            "    )\n",
            "\n",
            "# Title and labels\n",
            "plt.title('Number of Devices with and without Resets')\n",
            "plt.ylabel('Number of Devices')\n",
            "\n",
            "# Save the figure as a PNG \n",
            "output_path = \"../visualizations/plots/devices_with_without_resets.png\"\n",
            "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
            "\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Count the number of resets per device\n",
            "reset_counts = (\n",
            "    corrected_cum_daily_consumption\n",
            "    .filter(pl.col('reset_flag') == True)\n",
            "    .group_by('DEVICE_ID')\n",
            "    .count()\n",
            ")\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.hist(reset_counts['count'], bins=10, color='blue', edgecolor='black')\n",
            "plt.xlabel('Number of Resets')\n",
            "plt.ylabel('Number of Devices')\n",
            "plt.title('Distribution of Reset Counts per Device')\n",
            "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
            "plt.savefig(\"../visualizations/plots/reset_count_distribution.png\", dpi=300, bbox_inches='tight')\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Compute total reported days per device\n",
            "reported_days = corrected_cum_daily_consumption.group_by('DEVICE_ID').agg(\n",
            "    pl.count('reset_flag').alias('TOTAL_REPORTED_DAYS')\n",
            ")\n",
            "\n",
            "# Merge with reset counts\n",
            "reset_merged = reset_counts.join(reported_days, on='DEVICE_ID')\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.scatter(reset_merged['TOTAL_REPORTED_DAYS'], reset_merged['count'], alpha=0.7, color='purple')\n",
            "plt.xlabel('Total Reported Days per Device')\n",
            "plt.ylabel('Number of Resets')\n",
            "plt.title('Scatter Plot: Resets vs. Total Reported Days')\n",
            "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
            "plt.savefig(\"../visualizations/plots/resets_vs_reported_days.png\", dpi=300, bbox_inches='tight')\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Filter a single device with resets for visualizatio\n",
            "device_id = reset_counts['DEVICE_ID'][0]  # Pick a device with resets\n",
            "device_data = corrected_cum_daily_consumption.filter(pl.col('DEVICE_ID') == device_id)\n",
            "\n",
            "plt.figure(figsize=(12, 6))\n",
            "sns.lineplot(x=device_data['DATE'], y=device_data['ORIGINAL_CUMMULATIVE'], label=\"Corrected\")\n",
            "plt.scatter(\n",
            "    device_data.filter(pl.col('reset_flag') == True)['DATE'],\n",
            "    device_data.filter(pl.col('reset_flag') == True)['ORIGINAL_CUMMULATIVE'],\n",
            "    color='red', label=\"Reset Points\", zorder=3\n",
            ")\n",
            "plt.xlabel('Date')\n",
            "plt.ylabel('Cumulative Consumption')\n",
            "plt.title(f'Cumulative Consumption for Device {device_id} (Resets Highlighted)')\n",
            "plt.legend()\n",
            "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
            "plt.savefig(f\"../visualizations/plots/cumulative_resets_device_{device_id}.png\", dpi=300, bbox_inches='tight')\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "total_devices = len(corrected_cum_daily_consumption['DEVICE_ID'].unique())\n",
            "plt.figure(figsize=(8, 8))\n",
            "plt.pie(\n",
            "    [total_devices_reset, total_devices - total_devices_reset],\n",
            "    labels=[\"Devices with Resets\", \"Devices without Resets\"],\n",
            "    autopct='%1.1f%%',\n",
            "    colors=['red', 'green'],\n",
            "    startangle=40\n",
            ")\n",
            "plt.title(\"Proportion of Devices with and without Resets\")\n",
            "plt.savefig(\"../visualizations/plots/devices_with_without_resets_pie.png\", dpi=300, bbox_inches='tight')\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Validity Recheck"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corrected_daily_consumption=corrected_cum_daily_consumption.with_columns(\n",
            "        (pl.col(\"CUMMULATIVE_CONSUMPTION\") - pl.col(\"CUMMULATIVE_CONSUMPTION\").shift(1))\n",
            "        .over(\"DEVICE_ID\")\n",
            "        .alias(\"DAILY_CONSUMPTION\")\n",
            "    ).filter(pl.col(\"DAILY_CONSUMPTION\").is_not_null())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "corrected_daily_consumption.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 1: Flag invalid rows based on business rules\n",
            "validity_check = corrected_daily_consumption.with_columns(\n",
            "    pl.when(\n",
            "        (pl.col(\"CUMMULATIVE_CONSUMPTION\") == 0) | (pl.col(\"DAILY_CONSUMPTION\") < 0)\n",
            "    )\n",
            "    .then(0)  # Flag as True/1 if invalid\n",
            "    .otherwise(1)  # Otherwise False / 0\n",
            "    .alias(\"VALIDITY\")\n",
            ")\n",
            "validity_check.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "validity_check['VALIDITY'].value_counts()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "All records are now valid"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Accuracy"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# IQR method\n",
            "def detect_outliers_iqr(data):\n",
            "    q1 = np.percentile(data, 25)\n",
            "    q3 = np.percentile(data, 75)\n",
            "    iqr = q3 - q1\n",
            "    lower_bound = q1 - 1.5 * iqr\n",
            "    upper_bound = q3 + 1.5 * iqr\n",
            "    return (data < lower_bound) | (data > upper_bound)\n",
            "\n",
            "# MAD method\n",
            "def detect_outliers_mad(data, threshold=3):\n",
            "    median = np.median(data)\n",
            "    mad = np.median(np.abs(data - median))\n",
            "    modified_z_score = 0.6745 * (data - median) / mad\n",
            "    return np.abs(modified_z_score) > threshold\n",
            "\n",
            "# Isolation Forest method\n",
            "def detect_outliers_isolation_forest(data):\n",
            "    iso = IsolationForest(random_state=42,contamination='auto',)\n",
            "    return iso.fit_predict(data.reshape(-1, 1)) == -1\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Outlier detection workflow for a single device\n",
            "def detect_outliers_per_device(device_data):\n",
            "    # Mark `VALIDITY=0` as outliers automatically\n",
            "    device_data = device_data.with_columns(\n",
            "        pl.when(pl.col(\"VALIDITY\") == 0)\n",
            "        .then(True)  # Automatically mark as outlier\n",
            "        .otherwise(False)\n",
            "        .alias(\"FINAL_OUTLIER\")\n",
            "    )\n",
            "\n",
            "    # Filter out records with VALIDITY=0 before outlier detection\n",
            "    valid_data = device_data.filter(pl.col(\"VALIDITY\") == 1)\n",
            "\n",
            "    # Proceed only if valid data exists\n",
            "    if not valid_data.is_empty():\n",
            "      \n",
            "        data = valid_data[\"CUMMULATIVE_CONSUMPTION\"].to_numpy()\n",
            "\n",
            "        # Initialize empty columns for all possible outlier methods\n",
            "        valid_data = valid_data.with_columns(\n",
            "            # pl.lit(False).alias(\"OUTLIER_ZSCORE\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_IQR\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_MAD\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_ISO\"),\n",
            "        )\n",
            "\n",
            "        # Check normality\n",
            "        \n",
            "        iqr_outliers = detect_outliers_iqr(data)\n",
            "        mad_outliers = detect_outliers_mad(data)\n",
            "        iso_outliers = detect_outliers_isolation_forest(data)\n",
            "        \n",
            "        valid_data = valid_data.with_columns(\n",
            "            pl.Series(\"OUTLIER_ISO\", iso_outliers),\n",
            "            pl.Series(\"OUTLIER_MAD\", mad_outliers),\n",
            "            pl.Series(\"OUTLIER_IQR\", iqr_outliers),\n",
            "        )\n",
            "        # Combine Z-score and IQR using logical AND\n",
            "        final_outlier = np.logical_and(iqr_outliers, np.logical_and(mad_outliers, iso_outliers))\n",
            "\n",
            "        # Update FINAL_OUTLIER for valid records\n",
            "        valid_data = valid_data.with_columns(\n",
            "            pl.Series(\"FINAL_OUTLIER\", final_outlier)\n",
            "        )\n",
            "\n",
            "        # Add missing columns to invalid data with default values\n",
            "        invalid_data = device_data.filter(pl.col(\"VALIDITY\") == 0).with_columns(\n",
            "            # pl.lit(False).alias(\"OUTLIER_ZSCORE\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_IQR\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_MAD\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_ISO\"),\n",
            "        )\n",
            "\n",
            "        # Combine updated valid data with invalid data\n",
            "        device_data = valid_data.vstack(invalid_data)\n",
            "        \n",
            "    return device_data\n",
            "\n",
            "# Group data by DEVICE_ID and apply the workflow\n",
            "result = []\n",
            "for device_id, group in corrected_daily_consumption.group_by(\"DEVICE_ID\"):\n",
            "    processed_group = detect_outliers_per_device(group)\n",
            "    result.append(processed_group)\n",
            "\n",
            "# Concatenate results into a single DataFrame\n",
            "result_df = pl.concat(result)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df['OUTLIER_MAD'].value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df['OUTLIER_IQR'].value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df['OUTLIER_ISO'].value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df['FINAL_OUTLIER'].value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Outlier Detection on Original Cummulative"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Outlier detection workflow for a single device\n",
            "def detect_outliers_per_device(device_data):\n",
            "    # Mark `VALIDITY=0` as outliers automatically\n",
            "    device_data = device_data.with_columns(\n",
            "        pl.when(pl.col(\"VALIDITY\") == 0)\n",
            "        .then(True)  # Automatically mark as outlier\n",
            "        .otherwise(False)\n",
            "        .alias(\"FINAL_OUTLIER\")\n",
            "    )\n",
            "\n",
            "    # Filter out records with VALIDITY=0 before outlier detection\n",
            "    valid_data = device_data.filter(pl.col(\"VALIDITY\") == 1)\n",
            "\n",
            "    # Proceed only if valid data exists\n",
            "    if not valid_data.is_empty():\n",
            "      \n",
            "        data = valid_data[\"ORIGINAL_CUMMULATIVE\"].to_numpy()\n",
            "\n",
            "        # Initialize empty columns for all possible outlier methods\n",
            "        valid_data = valid_data.with_columns(\n",
            "            # pl.lit(False).alias(\"OUTLIER_ZSCORE\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_IQR\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_MAD\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_ISO\"),\n",
            "        )\n",
            "\n",
            "        iqr_outliers = detect_outliers_iqr(data)\n",
            "        mad_outliers = detect_outliers_mad(data)\n",
            "        iso_outliers = detect_outliers_isolation_forest(data)\n",
            "        \n",
            "        valid_data = valid_data.with_columns(\n",
            "            pl.Series(\"OUTLIER_ISO\", iso_outliers),\n",
            "            pl.Series(\"OUTLIER_MAD\", mad_outliers),\n",
            "            pl.Series(\"OUTLIER_IQR\", iqr_outliers),\n",
            "        )\n",
            "        # Combine Z-score and IQR using logical AND\n",
            "        final_outlier = np.logical_and(iqr_outliers, np.logical_and(mad_outliers, iso_outliers))\n",
            "\n",
            "        # Update FINAL_OUTLIER for valid records\n",
            "        valid_data = valid_data.with_columns(\n",
            "            pl.Series(\"FINAL_OUTLIER\", final_outlier)\n",
            "        )\n",
            "\n",
            "        # Add missing columns to invalid data with default values\n",
            "        invalid_data = device_data.filter(pl.col(\"VALIDITY\") == 0).with_columns(\n",
            "            pl.lit(False).alias(\"OUTLIER_IQR\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_MAD\"),\n",
            "            pl.lit(False).alias(\"OUTLIER_ISO\"),\n",
            "        )\n",
            "\n",
            "        # Combine updated valid data with invalid data\n",
            "        device_data = valid_data.vstack(invalid_data)\n",
            "        \n",
            "    return device_data\n",
            "\n",
            "# Group data by DEVICE_ID and apply the workflow\n",
            "result = []\n",
            "for device_id, group in corrected_daily_consumption.group_by(\"DEVICE_ID\"):\n",
            "    processed_group = detect_outliers_per_device(group)\n",
            "    result.append(processed_group)\n",
            "\n",
            "# Concatenate results into a single DataFrame\n",
            "original_result_df = pl.concat(result)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "original_result_df['FINAL_OUTLIER'].value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define anomaly counts before and after reconstruction\n",
            "anomaly_counts = {'Before Reconstruction': 2063, 'After Reconstruction': 1699}\n",
            "\n",
            "# Create a bar chart\n",
            "plt.figure(figsize=(8, 6))\n",
            "plt.bar(anomaly_counts.keys(), anomaly_counts.values(), color=['red', 'green'])\n",
            "plt.ylabel('Number of Anomalies')\n",
            "plt.title('Anomaly Count Before and After Reconstruction')\n",
            "\n",
            "# Annotate the bars with actual values\n",
            "for i, (label, count) in enumerate(anomaly_counts.items()):\n",
            "    plt.text(i, count + 5, str(count), ha='center', fontsize=12)\n",
            "\n",
            "# Save the figure as a PNG file\n",
            "plt.savefig(\"../visualizations/plots/anomaly_count_before_after_reconstruction.png\", dpi=300, bbox_inches='tight')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Visualization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# Step 1: Aggregate to count outliers per device\n",
            "outlier_summary = (\n",
            "    result_df\n",
            "    .filter(pl.col(\"FINAL_OUTLIER\") == 1)  # Filter rows marked as outliers\n",
            "    .group_by(\"DEVICE_ID\")\n",
            "    .agg(pl.count().alias(\"OUTLIER_COUNT\"))  # Count outliers per device\n",
            "    .sort(\"OUTLIER_COUNT\", descending=True)  # Sort devices by the number of outliers\n",
            ")\n",
            "\n",
            "# Step 2: Select top N devices with the most outliers\n",
            "top_n = 5  # Adjust this value as needed\n",
            "top_devices = outlier_summary.head(top_n)\n",
            "\n",
            "# Step 3: Filter data for the top devices\n",
            "top_device_ids = top_devices[\"DEVICE_ID\"].to_list()\n",
            "top_device_data = result_df.filter(pl.col(\"DEVICE_ID\").is_in(top_device_ids))\n",
            "\n",
            "\n",
            "for device_id in top_device_ids:\n",
            "    device_data = top_device_data.filter(pl.col(\"DEVICE_ID\") == device_id)\n",
            "    # Step 4: Visualize cumulative consumption for the top devices\n",
            "    plt.figure(figsize=(12, 8))\n",
            "    # Plot valid points\n",
            "    plt.plot(\n",
            "        device_data.filter(pl.col(\"FINAL_OUTLIER\") == 0)[\"DATE\"],\n",
            "        device_data.filter(pl.col(\"FINAL_OUTLIER\") == 0)[\"CUMMULATIVE_CONSUMPTION\"],\n",
            "        marker=\"o\",\n",
            "        label=f\"{device_id} - Valid\",\n",
            "        linestyle=\"-\",\n",
            "        alpha=0.8,\n",
            "    )\n",
            "    \n",
            "    # Plot outliers\n",
            "    plt.scatter(\n",
            "        device_data.filter(pl.col(\"FINAL_OUTLIER\") == 1)[\"DATE\"],\n",
            "        device_data.filter(pl.col(\"FINAL_OUTLIER\") == 1)[\"CUMMULATIVE_CONSUMPTION\"],\n",
            "        color=\"red\",\n",
            "        label=f\"{device_id} - Outlier\",\n",
            "        alpha=0.8,\n",
            "    )\n",
            "\n",
            "    # Add plot details\n",
            "    plt.title(\"Top Devices with the Most Outliers\", fontsize=14)\n",
            "    plt.xlabel(\"Date\", fontsize=12)\n",
            "    plt.ylabel(\"Cumulative Consumption\", fontsize=12)\n",
            "    plt.legend(fontsize=10, title=\"Legend\")\n",
            "    plt.grid(alpha=0.5, linestyle=\"--\")\n",
            "    plt.tight_layout()\n",
            "    \n",
            "    # Show plot\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Nullify column"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "### Keep a copy of daily_consumption before normalizing the column in place\n",
            "result_df = result_df.with_columns(pl.col('CUMMULATIVE_CONSUMPTION').alias('CUMMULATIVE_CONSUMPTION_COPY'))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 1: Create an Accuracy Column\n",
            "result_df = result_df.with_columns(\n",
            "    pl.when(pl.col(\"FINAL_OUTLIER\") == 0)\n",
            "    .then(1)  # Mark as accurate if FINAL_OUTLIER is 0\n",
            "    .otherwise(0)  # Mark as inaccurate if FINAL_OUTLIER is not 0\n",
            "    .alias(\"ACCURACY\")\n",
            ")\n",
            "\n",
            "# Step 2: Preview the DataFrame with the new Accuracy column\n",
            "result_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Nullify inaccurate records and impute them to increase completeness and accuracy."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df = result_df.with_columns(\n",
            "        pl.when(pl.col(\"FINAL_OUTLIER\"))\n",
            "        .then(None)  # Replace outliers with None (null)\n",
            "        .otherwise(pl.col(\"CUMMULATIVE_CONSUMPTION\"))\n",
            "        .alias(\"CUMMULATIVE_CONSUMPTION\")\n",
            "    )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cum_nan_count_column = result_df.select(\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").is_null().sum().alias(\"NaN_Count\")\n",
            ")\n",
            "print(cum_nan_count_column)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df=result_df.drop(['VALIDITY','ORIGINAL_CUMMULATIVE','reset_flag','FINAL_OUTLIER',\n",
            "                          'OUTLIER_IQR','OUTLIER_MAD','OUTLIER_ISO','ACCURACY','DAILY_DIFF'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df.shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "result_df['DEVICE_ID'].n_unique()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Device Selection\n",
            "\n",
            "Remove these device ids: 4759, 1307, 2049, 2048\n",
            "\n",
            "These devices either have constant and long period of no data or they are small in records"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "result_df =result_df.filter(pl.col('DEVICE_ID')!=1307)\n",
            "result_df = result_df.filter(pl.col('DEVICE_ID')!=2049)\n",
            "result_df = result_df.filter(pl.col('DEVICE_ID')!=2048)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Null Percentage\n",
            "\n",
            "Remove devices with small null percentage which can be ignored or deleted. So focus on devices with > 4% missing rates because the less than 4% are between 1 - 6 missing records. These are because during reconstruction anomaly did not detect them again."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 1: Add an `IS_NULL` column to identify null values\n",
            "result_df = result_df.with_columns(\n",
            "    pl.when(pl.col(\"CUMMULATIVE_CONSUMPTION\").is_null())\n",
            "    .then(1)\n",
            "    .otherwise(0)\n",
            "    .alias(\"IS_NULL\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 2: Calculate total rows and null rows per device\n",
            "null_percentage_df = result_df.group_by(\"DEVICE_ID\").agg(\n",
            "    pl.col(\"IS_NULL\").sum().alias(\"NULL_COUNT\"),  # Count of null values\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").count().alias(\"TOTAL_COUNT\")  # Total rows\n",
            ").with_columns(\n",
            "    # Step 3: Calculate null percentage\n",
            "    (pl.col(\"NULL_COUNT\") / pl.col(\"TOTAL_COUNT\") * 100).alias(\"NULL_PERCENTAGE\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "null_percentage_df.sort('NULL_PERCENTAGE', descending=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 4: Filter devices with missing rates over 4%\n",
            "high_null_devices = null_percentage_df.filter(pl.col(\"NULL_PERCENTAGE\") > 4).select(\"DEVICE_ID\")\n",
            "\n",
            "# Step 5: Keep only records from devices with high missing rates\n",
            "filtered_result_df = result_df.join(high_null_devices, on=\"DEVICE_ID\", how=\"inner\")\n",
            "\n",
            "# Check how many devices remain after filtering\n",
            "print(f\"Remaining devices with high missing rates: {filtered_result_df['DEVICE_ID'].n_unique()}\")\n",
            "\n",
            "# Display the first few rows of the filtered dataset\n",
            "filtered_result_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define counts for devices with outliers before and after reconstruction\n",
            "device_counts = {\n",
            "    'Before Reconstruction': 56,\n",
            "    # 'After Reconstruction': 55,\n",
            "    'Proceeding to Imputation': 19\n",
            "}\n",
            "\n",
            "# Create a bar chart\n",
            "plt.figure(figsize=(8, 6))\n",
            "plt.bar(device_counts.keys(), device_counts.values(), color=['red', 'green', 'blue'])\n",
            "plt.ylabel('Number of Devices with Outliers')\n",
            "plt.title('Devices with Outliers Before and After Reconstruction')\n",
            "\n",
            "# Annotate bars with actual values\n",
            "for i, (label, count) in enumerate(device_counts.items()):\n",
            "    plt.text(i, count + 0.5, str(count), ha='center', fontsize=12)\n",
            "\n",
            "# Save the figure as a PNG file\n",
            "plt.savefig(\"../visualizations/plots/devices_with_outliers_before_after.png\", dpi=300, bbox_inches='tight')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Imputation"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Statistics\n",
            "\n",
            "Mean, Median, Linear Interpolation, Cubic Interpolation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df=filtered_result_df\n",
            "result_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Mean"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df = result_df.with_columns([\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").fill_null(\n",
            "        pl.col(\"CUMMULATIVE_CONSUMPTION\").mean().over(\"DEVICE_ID\")\n",
            "    ).alias(\"MEAN_IMPUTED\")\n",
            "])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Median"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df = result_df.with_columns([\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").fill_null(\n",
            "        pl.col(\"CUMMULATIVE_CONSUMPTION\").median().over(\"DEVICE_ID\")\n",
            "    ).alias(\"MEDIAN_IMPUTED\")\n",
            "])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Linear Interpolation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Convert Polars DataFrame to Pandas for interpolation\n",
            "df_pd = result_df.to_pandas()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Interpolate by Linear Method, ensuring all missing values are filled\n",
            "df_pd[\"LINEAR_IMPUTED\"] = (\n",
            "    df_pd.groupby(\"DEVICE_ID\", group_keys=False)[\"CUMMULATIVE_CONSUMPTION\"]\n",
            "    .apply(lambda x: x.interpolate(method=\"linear\", limit_direction=\"both\"))\n",
            "    .reset_index(drop=True)  # Align index with the original DataFrame\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Cubic Interpolation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# Interpolate by Cubic Method\n",
            "df_pd[\"CUBIC_IMPUTED\"] = (\n",
            "    df_pd.groupby(\"DEVICE_ID\", group_keys=False)[\"CUMMULATIVE_CONSUMPTION\"]\n",
            "    .apply(lambda x: x.interpolate(method=\"cubic\",limit_direction=\"both\"))\n",
            "    .reset_index(drop=True)  # Align index with the original DataFrame\n",
            ")\n",
            "\n",
            "# Fill any remaining nulls using forward and backward fill\n",
            "df_pd[\"CUBIC_IMPUTED\"].fillna(method=\"ffill\", inplace=True)  # Fill forward\n",
            "df_pd[\"CUBIC_IMPUTED\"].fillna(method=\"bfill\", inplace=True)  # Fill backward\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Convert back to Polars\n",
            "result_df = pl.from_pandas(df_pd)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Confirm all records are imputed"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "null_count = df_pd[\"LINEAR_IMPUTED\"].isna().sum()\n",
            "print(f\"Total null values in LINEAR_IMPUTED: {null_count}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "null_count = df_pd[\"CUBIC_IMPUTED\"].isna().sum()\n",
            "print(f\"Total null values in CUBIC_IMPUTED: {null_count}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Naive"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Forward Fill"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Forward Fill\n",
            "result_df = result_df.with_columns(\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").forward_fill().alias(\"FFILL_IMPUTED\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Count null values in the \"FFILL_IMPUTED\" column\n",
            "null_count = result_df.select(pl.col(\"FFILL_IMPUTED\").is_null().sum()).item()\n",
            "print(f\"Total null values in FFILL_IMPUTED: {null_count}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df = result_df.with_columns(\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\")\n",
            "    .forward_fill()  \n",
            "    .backward_fill()# Fill any remaining nulls after backward fill\n",
            "    .alias(\"FFILL_IMPUTED\")\n",
            ")\n",
            "\n",
            "# Check if any nulls remain\n",
            "null_count = result_df.select(pl.col(\"BFILL_IMPUTED\").is_null().sum()).item()\n",
            "print(f\"Total null values in BFILL_IMPUTED after fixes: {null_count}\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Backward Fill"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Backward Fill\n",
            "result_df = result_df.with_columns(\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\").backward_fill().alias(\"BFILL_IMPUTED\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Count null values in the \"FFILL_IMPUTED\" column\n",
            "null_count = result_df.select(pl.col(\"BFILL_IMPUTED\").is_null().sum()).item()\n",
            "print(f\"Total null values in FFILL_IMPUTED: {null_count}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Use forward fill to fix the last one."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result_df = result_df.with_columns(\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION\")\n",
            "    .backward_fill()\n",
            "    .forward_fill()  # Fill any remaining nulls after backward fill\n",
            "    .alias(\"BFILL_IMPUTED\")\n",
            ")\n",
            "\n",
            "# Check if any nulls remain\n",
            "null_count = result_df.select(pl.col(\"BFILL_IMPUTED\").is_null().sum()).item()\n",
            "print(f\"Total null values in BFILL_IMPUTED after fixes: {null_count}\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### ML\n",
            "\n",
            "- KNN\n",
            "- LR\n",
            "- SAITS"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Load Exogenous parameters and merge\n",
            "- Climatic from GEE: https://code.earthengine.google.com/?accept_repo=users/jolaiyaemmanuel/thesis"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "climate_df = pl.read_csv('../data/daily_climate_gee.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "climate_df = climate_df.rename({'date': 'DATE'})\n",
            "\n",
            "# Convert the 'DATE' column in climate_df to datetime objects\n",
            "climate_df = climate_df.with_columns(pl.col('DATE').cast(pl.Date))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "climate_df = climate_df.drop('.geo')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Perform the join operation\n",
            "result_df = result_df.with_columns(pl.col('DATE').cast(pl.Date))\n",
            "merged_df = result_df.join(climate_df, on='DATE', how='left')\n",
            "\n",
            "# Print some info\n",
            "print(f\"Shape of the original DataFrame: {df.shape}\")\n",
            "print(f\"Shape of the climate DataFrame: {climate_df.shape}\")\n",
            "print(f\"Shape of the merged DataFrame: {merged_df.shape}\")\n",
            "\n",
            "merged_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Assuming your DataFrame `df` has these columns: TEMPERATURE_MEAN, TEMPERATURE_MIN, TEMPERATURE_MAX\n",
            "merged_df = merged_df.with_columns([\n",
            "    (pl.col(\"temp_max\") - 273.15).alias(\"temp_max\"),\n",
            "    (pl.col(\"temp_min\") - 273.15).alias(\"temp_min\"),\n",
            "])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Feature Engineering\n",
            "Only used in LR"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Temporal"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df= merged_df.with_columns(\n",
            "    pl.col('DATE').dt.weekday().alias('day_of_week'),\n",
            "    pl.col('DATE').dt.month().alias('month'),\n",
            "    pl.col('DATE').dt.week().alias('week_of_year'),\n",
            "    pl.col('DATE').dt.quarter().alias('quarter'),\n",
            "    pl.col('DATE').dt.day().alias('day'),\n",
            "    pl.col('DATE').dt.year().alias('year'),\n",
            "    # Add 'is_weekend' column: 1 if Saturday (5) or Sunday (6), else 0\n",
            "    pl.col('DATE').dt.weekday().is_in([5, 6]).cast(pl.Int8).alias('is_weekend')\n",
            ")\n",
            "\n",
            "merged_df = merged_df.with_columns([\n",
            "    np.sin(2 * np.pi * pl.col('day_of_week') / 7).alias('sin_day_of_week'),\n",
            "    np.cos(2 * np.pi * pl.col('day_of_week') / 7).alias('cos_day_of_week'),\n",
            "    np.sin(2 * np.pi * pl.col('month') / 12).alias('sin_month'),\n",
            "    np.cos(2 * np.pi * pl.col('month') / 12).alias('cos_month')\n",
            "])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Lags"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "lags = [1, 2, 3,]\n",
            "for lag in lags:\n",
            "    merged_df = merged_df.with_columns(\n",
            "        pl.col('CUMMULATIVE_CONSUMPTION').shift(lag).alias(f'lag_{lag}')\n",
            "    )\n",
            "    \n",
            "for lag in lags:\n",
            "    merged_df = merged_df.with_columns(\n",
            "        pl.col(f\"lag_{lag}\").fill_null(strategy=\"backward\")\n",
            "    )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Holiday"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# List of holiday dates\n",
            "holidays_2022 = [\n",
            "    '2022-01-01', '2022-04-15', '2022-04-16', '2022-04-17', '2022-04-18',\n",
            "    '2022-05-01', '2022-05-03', '2022-05-16', '2022-07-09', '2022-07-10',\n",
            "    '2022-07-30', '2022-12-24', '2022-12-25', '2022-12-26'\n",
            "]\n",
            "\n",
            "holidays_2023 = [\n",
            "    '2023-01-01', '2023-04-07', '2023-04-08', '2023-04-09', '2023-04-10',\n",
            "    '2023-04-21', '2023-05-01', '2023-05-16', '2023-06-28', '2023-07-09',\n",
            "    '2023-07-30', '2023-12-24', '2023-12-25', '2023-12-26'\n",
            "]\n",
            "\n",
            "holidays_2024 = [\n",
            "    '2024-01-01', '2024-01-09', '2024-03-29', '2024-03-30', '2024-03-31',\n",
            "    '2024-04-01', '2024-04-10', '2024-05-01', '2024-05-16', '2024-06-16',\n",
            "    '2024-07-09', '2024-07-30', '2024-12-24', '2024-12-25', '2024-12-26'\n",
            "]\n",
            "\n",
            "holiday_dates = holidays_2022 + holidays_2023 + holidays_2024\n",
            "\n",
            "holidays_df = pl.DataFrame({\n",
            "    'date': pl.Series(holiday_dates).cast(pl.Date),\n",
            "    'is_holiday': pl.Series([1] * len(holiday_dates))\n",
            "})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df = merged_df.join(\n",
            "    holidays_df,\n",
            "    left_on='DATE',\n",
            "    right_on='date',\n",
            "    how='left'\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Fill NaN values in 'is_holiday' with 0 (non-holiday)\n",
            "merged_df = merged_df.with_columns(\n",
            "    pl.col('is_holiday').fill_null(0)\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Normalizing\n",
            "Min-Max"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "normalized_df = merged_df.with_columns(\n",
            "    [\n",
            "        ((pl.col(col) - pl.col(col).min().over(\"DEVICE_ID\")) /\n",
            "         (pl.col(col).max().over(\"DEVICE_ID\") - pl.col(col).min().over(\"DEVICE_ID\")))\n",
            "        .alias(col)\n",
            "        for col in merged_df.columns\n",
            "        if col not in [\"DEVICE_ID\", \"DATE\",\"OGI_LONG\",\"OGI_LAT\",\n",
            "                       \"MEAN_IMPUTED\",\"MEDIAN_IMPUTED\",\"FFILL_IMPUTED\",\n",
            "                       \"BFILL_IMPUTED\",\"LINEAR_IMPUTED\",\n",
            "                       \"CUMMULATIVE_CONSUMPTION_COPY\",\n",
            "                      \"CUBIC_IMPUTED\"]  # Exclude non-numeric or identifier columns\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "normalized_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "normalized_df.filter(pl.col('CUMMULATIVE_CONSUMPTION').is_null()).head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Table - Missing rate"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Make a copy of the dataframe to avoid modifying the original\n",
            "numeric_df_pd_modified = normalized_df.to_pandas().copy()\n",
            "\n",
            "# Dictionary to store missing data statistics per device\n",
            "missing_data_per_device = []\n",
            "\n",
            "# Loop through each device and introduce 10% additional nulls\n",
            "for device_id, group in numeric_df_pd_modified.groupby(\"DEVICE_ID\"):\n",
            "    # Total number of records before introducing new nulls\n",
            "    total_records_before = len(group)\n",
            "    \n",
            "    # Count missing values before introducing new nulls\n",
            "    missing_before = group[\"CUMMULATIVE_CONSUMPTION\"].isna().sum()\n",
            "    \n",
            "    # Identify non-null indices for the current device\n",
            "    non_null_indices = group[group[\"CUMMULATIVE_CONSUMPTION\"].notna()].index\n",
            "    \n",
            "    # Determine the number of new nulls to introduce (10% of available non-null values)\n",
            "    num_new_nulls = int(0.1 * len(non_null_indices))\n",
            "    \n",
            "    if num_new_nulls > 0:\n",
            "        # Randomly select indices to introduce new nulls\n",
            "        new_null_indices = np.random.choice(non_null_indices, num_new_nulls, replace=False)\n",
            "        # Introduce the new nulls in the modified dataframe\n",
            "        numeric_df_pd_modified.loc[new_null_indices, \"CUMMULATIVE_CONSUMPTION\"] = np.nan\n",
            "    \n",
            "    # Count missing values after introducing new nulls\n",
            "    missing_after = numeric_df_pd_modified.loc[group.index, \"CUMMULATIVE_CONSUMPTION\"].isna().sum()\n",
            "    \n",
            "    # Total number of records should decrease due to introduced nulls\n",
            "    total_records_after = total_records_before - num_new_nulls\n",
            "    \n",
            "    # Store the results\n",
            "    missing_data_per_device.append({\n",
            "        \"DEVICE_ID\": device_id,\n",
            "        \"Total Records Before\": total_records_before,\n",
            "        \"Missing Before\": missing_before,\n",
            "        \"Total Records After\": total_records_after,\n",
            "        \"Missing After\": missing_after\n",
            "    })\n",
            "\n",
            "# Create a summary dataframe\n",
            "missing_data_summary_per_device = pd.DataFrame(missing_data_per_device)\n",
            "\n",
            "# Return the modified dataframe\n",
            "missing_data_summary_per_device.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "missing_data_summary_per_device"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Correlation Analysis"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Select numeric columns only\n",
            "numeric_columns = [col for col in normalized_df.columns if normalized_df[col].dtype in [pl.Float64, pl.Int64]]\n",
            "numeric_df = normalized_df.select(numeric_columns)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# List of exogenous features (replace with your feature names)\n",
            "exogenous_features = [\n",
            "    'precip_max',\n",
            "    'precip_min',\n",
            "    'temp_max',\n",
            "    'temp_min',\n",
            "    'day_of_week',\n",
            "    'month',\n",
            "    'week_of_year',\n",
            "    'quarter',\n",
            "    'day',\n",
            "    'year',\n",
            "    'is_weekend',\n",
            "    'sin_day_of_week',\n",
            "    'cos_day_of_week',\n",
            "    'sin_month',\n",
            "    'cos_month',\n",
            "    'lag_1',\n",
            "    'lag_2',\n",
            "    'lag_3',\n",
            "    'is_holiday'\n",
            "]\n",
            "\n",
            "# Manual method\n",
            "\n",
            "def compute_device_correlations_spearman(\n",
            "    df: pl.DataFrame, \n",
            "    target_column: str, \n",
            "    exogenous_features: list\n",
            ") -> pl.DataFrame:\n",
            "    # Get unique device IDs\n",
            "    device_ids = df['DEVICE_ID'].unique().to_list()\n",
            "    \n",
            "    # List to store correlation results\n",
            "    all_correlations = []\n",
            "    \n",
            "    # Compute correlations for each device\n",
            "    for device_id in device_ids:\n",
            "        # Filter data for the specific device\n",
            "        device_data = df.filter(pl.col('DEVICE_ID') == device_id)\n",
            "\n",
            "        # Compute Spearman correlation for each feature\n",
            "        # Spearman correlation = Pearson correlation on rank-transformed data\n",
            "        correlations = []\n",
            "        for feature in exogenous_features:\n",
            "            # Rank-transform both columns\n",
            "            ranked_feature = pl.col(feature).rank(\"average\")\n",
            "            ranked_target = pl.col(target_column).rank(\"average\")\n",
            "            \n",
            "            # Compute the correlation of these ranks\n",
            "            spearman_corr = device_data.select(\n",
            "                pl.corr(ranked_feature, ranked_target)\n",
            "            )[0, 0]\n",
            "            \n",
            "            correlations.append({\n",
            "                \"device_id\": device_id,\n",
            "                \"feature\": feature,\n",
            "                \"correlation\": spearman_corr\n",
            "            })\n",
            "        \n",
            "        all_correlations.extend(correlations)\n",
            "    \n",
            "    # Convert to Polars DataFrame\n",
            "    correlation_df = pl.DataFrame(all_correlations)\n",
            "    \n",
            "    return correlation_df\n",
            "\n",
            "# Usage\n",
            "correlation_results = compute_device_correlations_spearman(\n",
            "    numeric_df,\n",
            "    \"CUMMULATIVE_CONSUMPTION_COPY\",  # Replace with your target column\n",
            "    exogenous_features\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Step 2: Analyze the Results\n",
            "# Find the top correlated features across all devices\n",
            "top_features = (\n",
            "    correlation_results.group_by(\"feature\")\n",
            "    .agg(pl.col(\"correlation\").abs().mean().alias(\"mean_correlation\"))\n",
            "    .sort(\"mean_correlation\", descending=True)\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "top_features"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Colinearity"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Since we're using linear regression, we have to prevent collinearity."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "numeric_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Dictionary to store VIF results per device\n",
            "vif_per_device = []\n",
            "\n",
            "# Loop through each device and compute VIF\n",
            "for device_id, group in numeric_df.to_pandas().groupby(\"DEVICE_ID\"):\n",
            "    # Select lagged features\n",
            "    lags_df = group[[\"lag_1\", \"lag_2\", \"lag_3\"]].replace([np.inf, -np.inf], np.nan).dropna()\n",
            "\n",
            "    # Ensure there are enough observations to compute VIF\n",
            "    if lags_df.shape[0] > 3:  # VIF requires at least as many rows as columns\n",
            "        vif_data = pd.DataFrame()\n",
            "        vif_data[\"Feature\"] = lags_df.columns\n",
            "        vif_data[\"VIF\"] = [variance_inflation_factor(lags_df.values, i) for i in range(lags_df.shape[1])]\n",
            "        vif_data[\"DEVICE_ID\"] = device_id\n",
            "\n",
            "        # Append results\n",
            "        vif_per_device.append(vif_data)\n",
            "\n",
            "# Combine all device VIF results\n",
            "vif_results_df = pd.concat(vif_per_device, ignore_index=True)\n",
            "\n",
            "# Compute the mean VIF per feature across all devices\n",
            "average_vif = vif_results_df.groupby(\"Feature\")[\"VIF\"].mean().reset_index()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "average_vif"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Based on this, only lag 1 is kept."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "columns_to_remove = top_features.filter(pl.col('mean_correlation') < 0.5)\n",
            "# remove colinear features too\n",
            "cols_to_remove_list = columns_to_remove['feature'].to_list() + ['lag_2','lag_3']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cols_to_remove_list"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Filter features with mean correlation > 0.5\n",
            "exogenous_features = top_features.filter(pl.col('mean_correlation') > 0.5)\n",
            "\n",
            "# Convert to list and exclude 'lag_1', 'lag_2', and 'lag_3'\n",
            "exogenous_features = [feature for feature in exogenous_features['feature'].to_list() if feature not in [\"lag_2\", \"lag_3\"]]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "normalized_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelling_df = normalized_df.drop(cols_to_remove_list)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelling_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelling_df['DEVICE_ID'].n_unique(), modelling_df['DEVICE_ID'].shape[0]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### KNN"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelling_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# Define range of k values to test\n",
            "K_VALUES = list(range(1, 11))  # Testing k from 1 to 10\n",
            "\n",
            "# Columns used as input for KNN (but only imputing CUMMULATIVE_CONSUMPTION)\n",
            "target_column = \"CUMMULATIVE_CONSUMPTION\"\n",
            "feature_columns = [\"year\", \"lag_1\",]\n",
            "selected_features = [target_column] + feature_columns\n",
            "\n",
            "# Dictionary to store best k per device\n",
            "best_k_per_device = {}\n",
            "\n",
            "# List to store final imputed data\n",
            "imputed_data = []\n",
            "\n",
            "# Loop through each device group\n",
            "for device_id, group in modelling_df.group_by(\"DEVICE_ID\"):\n",
            "    print(f'Optimizing k for Device: {device_id}')\n",
            "\n",
            "    # Extract features as NumPy array\n",
            "    values = group.select(selected_features).to_numpy()\n",
            "\n",
            "    # Identify missing values in CUMMULATIVE_CONSUMPTION\n",
            "    missing_mask = np.isnan(values[:, 0])  # Only for target column\n",
            "\n",
            "    # Ensure enough non-null values for RMSE evaluation\n",
            "    if np.sum(~missing_mask) < 5:\n",
            "        print(f\"Skipping device {device_id} due to insufficient data for validation.\")\n",
            "        best_k_per_device[device_id] = None\n",
            "        continue\n",
            "\n",
            "    # Create a copy for evaluation (masking known values for validation)\n",
            "    known_values = values[:, 0].copy()  # Keep only the target column\n",
            "    known_indices = np.where(~missing_mask)[0]  # Indices of known values\n",
            "\n",
            "    if len(known_indices) > 5:  # Ensure at least 5 known values exist for testing\n",
            "        np.random.shuffle(known_indices)\n",
            "        test_indices = known_indices[: max(1, len(known_indices) // 10)]  # Mask 10% of known values for validation\n",
            "        test_mask = np.zeros_like(values[:, 0], dtype=bool)\n",
            "        test_mask[test_indices] = True\n",
            "\n",
            "        train_values = values.copy()\n",
            "        train_values[test_mask, 0] = np.nan  # Mask only `CUMMULATIVE_CONSUMPTION` for evaluation\n",
            "\n",
            "        # Optuna optimization\n",
            "        def objective(trial):\n",
            "            k = trial.suggest_int(\"k\", 1, 10)\n",
            "            knn_imputer = KNNImputer(n_neighbors=k)\n",
            "            imputed = knn_imputer.fit_transform(train_values)\n",
            "\n",
            "            # Compute RMSE only for masked `CUMMULATIVE_CONSUMPTION` values\n",
            "            rmse = np.sqrt(mean_squared_error(known_values[test_mask], imputed[test_mask, 0]))\n",
            "            return rmse\n",
            "\n",
            "        study = optuna.create_study(direction=\"minimize\")\n",
            "        study.optimize(objective, n_trials=10)\n",
            "\n",
            "        best_k = study.best_params[\"k\"]\n",
            "    else:\n",
            "        best_k = 5  # Default k if too few known values exist\n",
            "\n",
            "    best_k_per_device[device_id] = best_k  # Store best k for the device\n",
            "\n",
            "    # Perform final imputation using best k (only imputing target column)\n",
            "    knn_imputer = KNNImputer(n_neighbors=best_k)\n",
            "    final_imputed = knn_imputer.fit_transform(values)[:, 0]  # Only keep target column\n",
            "\n",
            "    # Append imputed values to group without altering other features\n",
            "    group = group.with_columns(pl.Series(\"KNN_IMPUTED\", final_imputed))\n",
            "\n",
            "    imputed_data.append(group)\n",
            "\n",
            "# Combine all processed groups\n",
            "imputed_df = pl.concat(imputed_data)\n",
            "\n",
            "# Print best k values per device\n",
            "print(\"Best k per device:\", best_k_per_device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "imputed_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### LR"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### LR + TSSplit"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define target column and features\n",
            "target_column = \"CUMMULATIVE_CONSUMPTION\"\n",
            "feature_columns = [\"year\", \"lag_1\"]\n",
            "features = feature_columns\n",
            "\n",
            "# List to store imputed data for all devices\n",
            "all_devices = []\n",
            "\n",
            "# Get unique device IDs\n",
            "device_ids = imputed_df[\"DEVICE_ID\"].unique().to_list()\n",
            "\n",
            "# Loop through each device\n",
            "for device_id in device_ids:\n",
            "    # Filter data for the current device\n",
            "    device_data = imputed_df.filter(pl.col(\"DEVICE_ID\") == device_id)\n",
            "\n",
            "    # Separate rows with missing and non-missing target values\n",
            "    non_missing_df = device_data.filter(~pl.col(target_column).is_null())\n",
            "    missing_df = device_data.filter(pl.col(target_column).is_null())\n",
            "\n",
            "    # Skip the device if there are no missing values\n",
            "    if missing_df.is_empty():\n",
            "        all_devices.append(device_data.with_columns(\n",
            "            pl.col(target_column).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "        ))\n",
            "        continue\n",
            "\n",
            "    # Prepare training data (features and target) for Linear Regression\n",
            "    X_train = non_missing_df.select(features).to_pandas()\n",
            "    y_train = non_missing_df.select(target_column).to_pandas().values.ravel()\n",
            "\n",
            "    # Prepare data for prediction (features only)\n",
            "    X_predict = missing_df.select(features).to_pandas()\n",
            "\n",
            "    #  Fix: Impute missing feature values (replace NaNs with mean) especially for the lags\n",
            "    imputer = SimpleImputer(strategy=\"mean\")\n",
            "    X_train = imputer.fit_transform(X_train)\n",
            "    X_predict = imputer.transform(X_predict)\n",
            "\n",
            "    # Time Series Cross-Validation\n",
            "    tscv = TimeSeriesSplit(n_splits=5)\n",
            "    rmse_scores = []\n",
            "\n",
            "    for train_idx, test_idx in tscv.split(X_train):\n",
            "        X_tr, X_val = X_train[train_idx], X_train[test_idx]\n",
            "        y_tr, y_val = y_train[train_idx], y_train[test_idx]\n",
            "\n",
            "        # Train Linear Regression model\n",
            "        model = LinearRegression()\n",
            "        model.fit(X_tr, y_tr)\n",
            "\n",
            "        # Predict on validation set\n",
            "        y_pred = model.predict(X_val)\n",
            "\n",
            "        # Compute RMSE for validation set\n",
            "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
            "        rmse_scores.append(rmse)\n",
            "\n",
            "    # Compute average RMSE across folds\n",
            "    mean_rmse = np.mean(rmse_scores)\n",
            "    print(f\"Device {device_id} - Mean RMSE: {mean_rmse}\")\n",
            "\n",
            "    # Train final model on entire available dataset\n",
            "    final_model = LinearRegression()\n",
            "    final_model.fit(X_train, y_train)\n",
            "\n",
            "    # Predict missing values\n",
            "    predicted_values = final_model.predict(X_predict)\n",
            "\n",
            "    # Add the predicted values to the missing_df in a new column\n",
            "    missing_df = missing_df.with_columns(\n",
            "        pl.Series(predicted_values).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "    )\n",
            "\n",
            "    # For non-missing rows, copy the original target column into the new column\n",
            "    non_missing_df = non_missing_df.with_columns(\n",
            "        pl.col(target_column).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "    )\n",
            "\n",
            "    # Combine the non-missing and imputed data for the current device\n",
            "    device_combined = pl.concat([non_missing_df, missing_df]).sort(\"DATE\")\n",
            "    all_devices.append(device_combined)\n",
            "\n",
            "# Combine data from all devices\n",
            "final_df = pl.concat(all_devices)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Define your target column and features\n",
            "# target_column = \"CUMMULATIVE_CONSUMPTION\"\n",
            "# feature_columns = [\"year\", \"lag_1\",]\n",
            "# features = feature_columns\n",
            "\n",
            "# # List to store imputed data for all devices\n",
            "# all_devices = []\n",
            "\n",
            "# # Get unique device IDs\n",
            "# device_ids = imputed_df[\"DEVICE_ID\"].unique().to_list()\n",
            "\n",
            "# # Loop through each device\n",
            "# for device_id in device_ids:\n",
            "#     # Filter data for the current device\n",
            "#     device_data = imputed_df.filter(pl.col(\"DEVICE_ID\") == device_id)\n",
            "\n",
            "#     # Separate rows with missing and non-missing target values\n",
            "#     non_missing_df = device_data.filter(~pl.col(target_column).is_null())\n",
            "#     missing_df = device_data.filter(pl.col(target_column).is_null())\n",
            "\n",
            "#     # Skip the device if there are no missing values\n",
            "#     if missing_df.is_empty():\n",
            "#         # Add non-missing data to the final list as is\n",
            "#         all_devices.append(device_data.with_columns(\n",
            "#             pl.col(target_column).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "#         ))\n",
            "#         continue\n",
            "\n",
            "#     # Prepare training data (features and target) for Linear Regression\n",
            "#     X_train = non_missing_df.select(features).to_pandas()\n",
            "#     y_train = non_missing_df.select(target_column).to_pandas().values.ravel()\n",
            "\n",
            "#     # Prepare data for prediction (features only)\n",
            "#     X_predict = missing_df.select(features).to_pandas()\n",
            "\n",
            "#     # Train Linear Regression model\n",
            "#     lr_model = LinearRegression()\n",
            "#     lr_model.fit(X_train, y_train)\n",
            "\n",
            "#     # Predict missing values\n",
            "#     predicted_values = lr_model.predict(X_predict)\n",
            "\n",
            "#     # Add the predicted values to the missing_df in a new column\n",
            "#     missing_df = missing_df.with_columns(\n",
            "#         pl.Series(predicted_values).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "#     )\n",
            "\n",
            "#     # For non-missing rows, copy the original target column into the new column\n",
            "#     non_missing_df = non_missing_df.with_columns(\n",
            "#         pl.col(target_column).alias(\"LIN_REG_IMPUTED_EXO\")\n",
            "#     )\n",
            "\n",
            "#     # Combine the non-missing and imputed data for the current device\n",
            "#     device_combined = pl.concat([non_missing_df, missing_df]).sort(\"DATE\")\n",
            "#     all_devices.append(device_combined)\n",
            "\n",
            "# # Combine data from all devices\n",
            "# final_df = pl.concat(all_devices)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "jp-MarkdownHeadingCollapsed": true
         },
         "source": [
            "### SAITS"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# final_df = final_df.with_columns(pl.col('DATE').cast(pl.Date))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# final_df = final_df.sort(['DEVICE_ID','DATE'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# final_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Add an index column to your Polars DataFrame\n",
            "# final_df = final_df.with_row_count(name=\"index\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "# # Define the feature columns to include\n",
            "# feature_columns = [\"CUMMULATIVE_CONSUMPTION\"]\n",
            "\n",
            "# # Dictionary to store imputed values for each device\n",
            "# imputed_values_dict = {}\n",
            "\n",
            "# for id, device in enumerate(final_df['DEVICE_ID'].unique()):\n",
            "#     print(f\"Processing device: {id + 1} of {len(final_df['DEVICE_ID'].unique())}\")\n",
            "\n",
            "#     # Filter and sort data for the device\n",
            "#     device_data = final_df.filter(pl.col('DEVICE_ID') == device).sort('DATE')\n",
            "\n",
            "#     # Select relevant columns and convert to NumPy array\n",
            "#     data_for_imputation = device_data.select(feature_columns).to_numpy()\n",
            "\n",
            "#     # Define X for SAITS\n",
            "#     X = data_for_imputation.reshape(1, -1, len(feature_columns))\n",
            "\n",
            "#     # Keep a copy of the original data for evaluation\n",
            "#     X_ori = X.copy()\n",
            "\n",
            "#     # Apply MCAR (mask 10% of the observed values for validation)\n",
            "#     X = mcar(X, 0.1)  # Mask 10% of the observed data\n",
            "\n",
            "#     dataset = {\"X\": X}\n",
            "\n",
            "#     # Initialize the SAITS model\n",
            "#     saits = SAITS(\n",
            "#         n_steps=X.shape[1],  # Number of time steps\n",
            "#         n_features=X.shape[2],  # Number of features\n",
            "#         n_layers=2,\n",
            "#         d_model=256,\n",
            "#         d_ffn=128,\n",
            "#         n_heads=4,\n",
            "#         d_k=64,\n",
            "#         d_v=64,\n",
            "#         dropout=0.1,\n",
            "#         epochs=100,\n",
            "#         batch_size=32,\n",
            "#         attn_dropout=0.1,\n",
            "#         patience=3,\n",
            "#         saving_path=\"./saits\",  # Path for saving model checkpoints\n",
            "#         model_saving_strategy=\"best\",\n",
            "#     )\n",
            "\n",
            "#     # Train the model and impute the missing values\n",
            "#     saits.fit(dataset)\n",
            "#     imputation = saits.impute(dataset)\n",
            "\n",
            "#     # Extract imputed values for the target column (CUMMULATIVE_CONSUMPTION)\n",
            "#     imputed_values = imputation[0, :, 0]  # Extract the imputed values for the first feature\n",
            "\n",
            "#     # Store the imputed values in the dictionary\n",
            "#     imputed_values_dict[device] = {\n",
            "#         \"indices\": device_data['index'].to_numpy(),  # Save indices for mapping back to original data\n",
            "#         \"imputed_values\": imputed_values,\n",
            "#     }\n",
            "\n",
            "#     # Calculate MAE and RMSE\n",
            "#     indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # Mask indicating artificially missing values\n",
            "#     mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)\n",
            "#     rmse = calc_rmse(imputation, np.nan_to_num(X_ori), indicating_mask)\n",
            "\n",
            "#     print(f\"Device {device} - MAE: {mae}\")\n",
            "#     print(f\"Device {device} - RMSE: {rmse}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Initialize an empty list to collect rows\n",
            "# rows = []\n",
            "\n",
            "# # Iterate over each device in the dictionary\n",
            "# for device_id, data in imputed_values_dict.items():\n",
            "#     indices = data[\"indices\"]\n",
            "#     imputed_values = data[\"imputed_values\"]\n",
            "\n",
            "#     # Create a row for each index and value\n",
            "#     for idx, value in zip(indices, imputed_values):\n",
            "#         rows.append({\"DEVICE_ID\": device_id, \"index\": idx, \"imputed_value\": value})\n",
            "\n",
            "# # Create a DataFrame from the collected rows\n",
            "# imputed_df = pd.DataFrame(rows)\n",
            "\n",
            "# # Sort the DataFrame by index if needed\n",
            "# imputed_df = imputed_df.sort_values(\"index\").reset_index(drop=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# imputed_df = imputed_df.sort_values(\"index\").reset_index(drop=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# imputed_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# imputed_df = pl.from_dataframe(imputed_df)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# imputed_df = imputed_df.with_columns(pl.col('index').cast(pl.UInt32))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# merged_df = final_df.join(imputed_df, on=[\"DEVICE_ID\",\"index\"], how=\"left\")\n",
            "# merged_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Create an array of 10 random device IDs from the unique device IDs in your dataset\n",
            "random_devices = random.sample(imputed_df['DEVICE_ID'].unique().to_list(), 10)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "\n",
            "for device in random_devices:  # Use the same random devices as before\n",
            "    device_data = imputed_df.filter(pl.col('DEVICE_ID') == device).to_pandas()\n",
            "    \n",
            "    plt.figure(figsize=(10, 6))\n",
            "\n",
            "    # Plot original cumulative consumption (with outliers removed)\n",
            "    plt.plot(device_data['DATE'], device_data['CUMMULATIVE_CONSUMPTION'], \n",
            "             linestyle='-', color='blue', label='Original (No Outliers)')\n",
            "\n",
            "    \n",
            "    # Plot KNN values\n",
            "    plt.plot(device_data['DATE'], device_data['KNN_IMPUTED'], \n",
            "             linestyle='--', color='red', label='KNN Imputed Values')\n",
            "    plt.xlabel('Date')\n",
            "    plt.ylabel('Cumulative Consumption')\n",
            "    plt.title(f'Cumulative Consumption with Imputed Values for Device ID: {device}')\n",
            "    plt.xticks(rotation=45)\n",
            "    plt.legend()\n",
            "    plt.tight_layout()\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Data Reconstruction"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "final_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Reverse Normalization"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# List of features to reverse normalize\n",
            "features_reverse_normalize = [\n",
            "    'KNN_IMPUTED',\n",
            "    'LIN_REG_IMPUTED_EXO',\n",
            "    # 'imputed_values'\n",
            "]\n",
            "\n",
            "scaling_params = final_df.group_by(\"DEVICE_ID\").agg([\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION_COPY\").min().alias(\"DAILY_CONSUMPTION_min\"),\n",
            "    pl.col(\"CUMMULATIVE_CONSUMPTION_COPY\").max().alias(\"DAILY_CONSUMPTION_max\")\n",
            "])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "scaling_params.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Join scaling params with merged_df\n",
            "merged_df = final_df.join(scaling_params, on=\"DEVICE_ID\", how=\"left\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Denormalize KNN predictions\n",
            "merged_df= merged_df.with_columns(\n",
            "    (\n",
            "        (pl.col(\"KNN_IMPUTED\") * \n",
            "         (pl.col(\"DAILY_CONSUMPTION_max\") - pl.col(\"DAILY_CONSUMPTION_min\"))) +\n",
            "        pl.col(\"DAILY_CONSUMPTION_min\")\n",
            "    ).alias(\"DENORMALIZED_KNN_IMPUTED\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Denormalize LR predictions\n",
            "merged_df = merged_df.with_columns(\n",
            "    (\n",
            "        (pl.col(\"LIN_REG_IMPUTED_EXO\") * \n",
            "         (pl.col(\"DAILY_CONSUMPTION_max\") - pl.col(\"DAILY_CONSUMPTION_min\"))) +\n",
            "        pl.col(\"DAILY_CONSUMPTION_min\")\n",
            "    ).alias(\"DENORMALIZED_LIN_REG_IMPUTED\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Denormalize SAITS predictions\n",
            "# merged_df = merged_df.with_columns(\n",
            "#     (\n",
            "#         (pl.col(\"imputed_value\") * \n",
            "#          (pl.col(\"DAILY_CONSUMPTION_max\") - pl.col(\"DAILY_CONSUMPTION_min\"))) +\n",
            "#         pl.col(\"DAILY_CONSUMPTION_min\")\n",
            "#     ).alias(\"DENORMALIZED_SAITS_IMPUTED\")\n",
            "# )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merged_df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "reconstructed_df = merged_df.select([\n",
            "    \"DATE\",\"DEVICE_ID\",\"OGI_LONG\",\"OGI_LAT\",\n",
            "    \"MEAN_IMPUTED\",\"MEDIAN_IMPUTED\",\"FFILL_IMPUTED\",\"BFILL_IMPUTED\",\"LINEAR_IMPUTED\",\"CUBIC_IMPUTED\",\n",
            "    \"DENORMALIZED_KNN_IMPUTED\",\"CUMMULATIVE_CONSUMPTION_COPY\",\"DENORMALIZED_LIN_REG_IMPUTED\",\n",
            "])\n",
            "\n",
            "# \"DENORMALIZED_SAITS_IMPUTED\","
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "reconstructed_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Export imputed data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "reconstructed_df.write_csv('../exports/imputed_water_meters_v2.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "reconstructed_df['DEVICE_ID'].n_unique()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "colab": {
         "collapsed_sections": [
            "hyDedPc8HvSa",
            "3VpS-O9vHvSb",
            "J4tcXDKnhh6q",
            "xA9no9XJPl2y",
            "m1HOmuf1HvSo",
            "rj5QHprRHvSo",
            "sj2dbOvFHvSp",
            "JnRkg4dbHvSr",
            "gPGlpPwjr3n2",
            "bB1UKw1osLp7"
         ],
         "provenance": [],
         "toc_visible": true
      },
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.15"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
